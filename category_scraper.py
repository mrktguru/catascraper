#!/usr/bin/env python3
"""
Catawiki Category Scraper - –ø–∞—Ä—Å–∏–Ω–≥ –≤—Å–µ—Ö –ª–æ—Ç–æ–≤ –≤ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ —Å –ø–∞–≥–∏–Ω–∞—Ü–∏–µ–π
"""

import asyncio
import time
from typing import List, Optional
from playwright.async_api import async_playwright
from scraper_pro import CatawikiScraperPro


class CatawikiCategoryScraper:
    def __init__(self, headless: bool = True):
        self.headless = headless
        self.scraper = CatawikiScraperPro(headless=headless)

    async def extract_lot_urls_from_page(self, page) -> List[str]:
        """–ò–∑–≤–ª–µ—á—å –≤—Å–µ URL –ª–æ—Ç–æ–≤ —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∫–∞—Ç–µ–≥–æ—Ä–∏–∏"""
        lot_urls = []

        try:
            # –ù–∞–π—Ç–∏ –≤—Å–µ –∫–∞—Ä—Ç–æ—á–∫–∏ –ª–æ—Ç–æ–≤
            lot_cards = await page.query_selector_all('[data-testid^="lot-card-container-"]')
            print(f"[{time.strftime('%H:%M:%S')}] –ù–∞–π–¥–µ–Ω–æ {len(lot_cards)} –∫–∞—Ä—Ç–æ—á–µ–∫ –ª–æ—Ç–æ–≤")

            for card in lot_cards:
                # –ù–∞–π—Ç–∏ —Å—Å—ã–ª–∫—É –≤–Ω—É—Ç—Ä–∏ –∫–∞—Ä—Ç–æ—á–∫–∏
                link = await card.query_selector('a[href*="/en/l/"]')
                if link:
                    href = await link.get_attribute('href')
                    if href:
                        # –ü–æ–ª–Ω—ã–π URL
                        if href.startswith('http'):
                            lot_url = href.split('?')[0]  # –£–±—Ä–∞—Ç—å query –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
                        else:
                            lot_url = f"https://www.catawiki.com{href.split('?')[0]}"

                        lot_urls.append(lot_url)

        except Exception as e:
            print(f"[{time.strftime('%H:%M:%S')}] ‚ùå –û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è URL: {e}")

        return lot_urls

    async def get_total_pages(self, page) -> int:
        """–û–ø—Ä–µ–¥–µ–ª–∏—Ç—å –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü –≤ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏"""
        try:
            # –ù–∞–π—Ç–∏ –Ω–∞–≤–∏–≥–∞—Ü–∏—é –ø–∞–≥–∏–Ω–∞—Ü–∏–∏
            pagination = await page.query_selector('nav.c-pagination__container')
            if not pagination:
                return 1

            # –ù–∞–π—Ç–∏ –≤—Å–µ –Ω–æ–º–µ—Ä–∞ —Å—Ç—Ä–∞–Ω–∏—Ü
            page_numbers = await pagination.query_selector_all('[data-testid="page"]')

            max_page = 1
            for page_elem in page_numbers:
                text = await page_elem.inner_text()
                text = text.strip()

                # –ü—Ä–æ–ø—É—Å—Ç–∏—Ç—å "..."
                if text == '‚Ä¶' or text == '...':
                    continue

                try:
                    page_num = int(text)
                    if page_num > max_page:
                        max_page = page_num
                except ValueError:
                    continue

            print(f"[{time.strftime('%H:%M:%S')}] üìÑ –í—Å–µ–≥–æ —Å—Ç—Ä–∞–Ω–∏—Ü: {max_page}")
            return max_page

        except Exception as e:
            print(f"[{time.strftime('%H:%M:%S')}] ‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü: {e}")
            return 1

    async def scrape_category(self, category_url: str, max_pages: Optional[int] = None) -> List[dict]:
        """
        –ü–∞—Ä—Å–∏–Ω–≥ –≤—Å–µ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ —Å –ø–∞–≥–∏–Ω–∞—Ü–∏–µ–π

        Args:
            category_url: URL –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
            max_pages: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ (None = –≤—Å–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã)

        Returns:
            –°–ø–∏—Å–æ–∫ –¥–∞–Ω–Ω—ã—Ö –≤—Å–µ—Ö –ª–æ—Ç–æ–≤
        """
        print("=" * 70)
        print("üóÇÔ∏è Catawiki Category Scraper")
        print("=" * 70)
        print(f"Category URL: {category_url}")
        print(f"Max pages: {max_pages or 'ALL'}")
        print(f"Time: {time.strftime('%Y-%m-%d %H:%M:%S')}")
        print("=" * 70)

        all_lot_urls = []

        async with async_playwright() as p:
            try:
                # –ó–∞–ø—É—Å—Ç–∏—Ç—å –±—Ä–∞—É–∑–µ—Ä
                browser = await p.chromium.launch(
                    headless=self.headless,
                    args=[
                        '--disable-blink-features=AutomationControlled',
                        '--disable-dev-shm-usage',
                        '--no-sandbox',
                    ]
                )

                page = await browser.new_page()

                # –ó–∞–≥—Ä—É–∑–∏—Ç—å –ø–µ—Ä–≤—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
                print(f"[{time.strftime('%H:%M:%S')}] üåê –ó–∞–≥—Ä—É–∑–∫–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏...")
                await page.goto(category_url, wait_until='domcontentloaded', timeout=30000)
                await asyncio.sleep(3)  # –î–∞—Ç—å –≤—Ä–µ–º—è –Ω–∞ –∑–∞–≥—Ä—É–∑–∫—É –∫–æ–Ω—Ç–µ–Ω—Ç–∞

                # –û–ø—Ä–µ–¥–µ–ª–∏—Ç—å –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü
                total_pages = await self.get_total_pages(page)

                if max_pages:
                    total_pages = min(total_pages, max_pages)

                # –ü–∞—Ä—Å–∏–Ω–≥ –∫–∞–∂–¥–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
                for page_num in range(1, total_pages + 1):
                    print(f"\n[{time.strftime('%H:%M:%S')}] üìë –°—Ç—Ä–∞–Ω–∏—Ü–∞ {page_num}/{total_pages}")

                    # –ï—Å–ª–∏ –Ω–µ –ø–µ—Ä–≤–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞, –ø–µ—Ä–µ–π—Ç–∏ –Ω–∞ –Ω—É–∂–Ω—É—é
                    if page_num > 1:
                        # –ü–æ—Å—Ç—Ä–æ–∏—Ç—å URL —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–º page
                        separator = '&' if '?' in category_url else '?'
                        page_url = f"{category_url}{separator}page={page_num}"

                        print(f"[{time.strftime('%H:%M:%S')}] üåê –ü–µ—Ä–µ—Ö–æ–¥ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—É {page_num}...")
                        await page.goto(page_url, wait_until='domcontentloaded', timeout=30000)
                        await asyncio.sleep(3)

                    # –ò–∑–≤–ª–µ—á—å URL –ª–æ—Ç–æ–≤ —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
                    lot_urls = await self.extract_lot_urls_from_page(page)
                    all_lot_urls.extend(lot_urls)

                    print(f"[{time.strftime('%H:%M:%S')}] ‚úì –ò–∑–≤–ª–µ—á–µ–Ω–æ {len(lot_urls)} URL –ª–æ—Ç–æ–≤")

                await browser.close()

            except Exception as e:
                print(f"[{time.strftime('%H:%M:%S')}] ‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏: {e}")
                return []

        # –£–¥–∞–ª–∏—Ç—å –¥—É–±–ª–∏–∫–∞—Ç—ã
        all_lot_urls = list(set(all_lot_urls))
        print(f"\n[{time.strftime('%H:%M:%S')}] üìä –í—Å–µ–≥–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –ª–æ—Ç–æ–≤: {len(all_lot_urls)}")

        # –¢–µ–ø–µ—Ä—å –ø–∞—Ä—Å–∏–º –∫–∞–∂–¥—ã–π –ª–æ—Ç
        print(f"\n[{time.strftime('%H:%M:%S')}] üöÄ –ù–∞—á–∏–Ω–∞–µ–º –ø–∞—Ä—Å–∏–Ω–≥ –∫–∞–∂–¥–æ–≥–æ –ª–æ—Ç–∞...")
        all_results = []

        for i, lot_url in enumerate(all_lot_urls, 1):
            print(f"\n[{time.strftime('%H:%M:%S')}] üì¶ –õ–æ—Ç {i}/{len(all_lot_urls)}: {lot_url}")

            try:
                # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π scraper –¥–ª—è –ª–æ—Ç–∞
                result = await self.scraper.scrape_listing(lot_url)

                if result and result.get('title'):
                    all_results.append(result)
                    print(f"[{time.strftime('%H:%M:%S')}] ‚úÖ –£—Å–ø–µ—à–Ω–æ —Å–ø–∞—Ä—Å–µ–Ω–æ")
                else:
                    print(f"[{time.strftime('%H:%M:%S')}] ‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Å–ø–∞—Ä—Å–∏—Ç—å –ª–æ—Ç")

            except Exception as e:
                print(f"[{time.strftime('%H:%M:%S')}] ‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –ª–æ—Ç–∞: {e}")
                continue

            # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
            if i < len(all_lot_urls):
                await asyncio.sleep(3)

        print("\n" + "=" * 70)
        print(f"‚úÖ –ü–∞—Ä—Å–∏–Ω–≥ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –∑–∞–≤–µ—Ä—à–µ–Ω!")
        print(f"–í—Å–µ–≥–æ –ª–æ—Ç–æ–≤ –Ω–∞–π–¥–µ–Ω–æ: {len(all_lot_urls)}")
        print(f"–£—Å–ø–µ—à–Ω–æ —Å–ø–∞—Ä—Å–µ–Ω–æ: {len(all_results)}")
        print(f"–ü—Ä–æ–≤–∞–ª–µ–Ω–æ: {len(all_lot_urls) - len(all_results)}")
        print("=" * 70)

        return all_results


async def main():
    """–ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è"""
    import sys
    import json

    if len(sys.argv) < 2:
        print("Usage: python category_scraper.py <category_url> [max_pages]")
        print("\nExample:")
        print('  python category_scraper.py "https://www.catawiki.com/en/s?q=burgundy&filters=..." 2')
        sys.exit(1)

    category_url = sys.argv[1]
    max_pages = int(sys.argv[2]) if len(sys.argv) > 2 else None

    scraper = CatawikiCategoryScraper(headless=True)
    results = await scraper.scrape_category(category_url, max_pages=max_pages)

    # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    if results:
        output_file = f"category_results_{int(time.time())}.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)

        print(f"\nüíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {output_file}")


if __name__ == "__main__":
    asyncio.run(main())
