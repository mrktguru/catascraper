#!/usr/bin/env python3
"""
Catawiki Category Scraper - –ø–∞—Ä—Å–∏–Ω–≥ –≤—Å–µ—Ö –ª–æ—Ç–æ–≤ –≤ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ —Å –ø–∞–≥–∏–Ω–∞—Ü–∏–µ–π
"""

import asyncio
import time
from typing import List, Optional
from playwright.async_api import async_playwright
from scraper_pro import CatawikiScraperPro


class CatawikiCategoryScraper:
    def __init__(self, headless: bool = True):
        print(f"[DEBUG] CatawikiCategoryScraper.__init__ received headless={headless!r}, type={type(headless)}")
        self.headless = headless
        self.scraper = CatawikiScraperPro(headless=headless)

    async def extract_lot_urls_from_page(self, page) -> List[str]:
        """–ò–∑–≤–ª–µ—á—å –≤—Å–µ URL –ª–æ—Ç–æ–≤ —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∫–∞—Ç–µ–≥–æ—Ä–∏–∏"""
        lot_urls = []

        try:
            # –ü–æ–ø—Ä–æ–±—É–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ —Å–µ–ª–µ–∫—Ç–æ—Ä–æ–≤
            selectors = [
                '[data-testid^="lot-card-container-"]',
                'article.c-lot-card__container',
                'a.c-lot-card[href*="/en/l/"]',
                '[data-sentry-component="ListingLotsWrapper"] a[href*="/en/l/"]'
            ]

            lot_cards = []
            for selector in selectors:
                lot_cards = await page.query_selector_all(selector)
                print(f"[{time.strftime('%H:%M:%S')}] –°–µ–ª–µ–∫—Ç–æ—Ä '{selector}': –Ω–∞–π–¥–µ–Ω–æ {len(lot_cards)} —ç–ª–µ–º–µ–Ω—Ç–æ–≤")
                if lot_cards:
                    break

            if not lot_cards:
                print(f"[{time.strftime('%H:%M:%S')}] ‚ö†Ô∏è –ù–µ –Ω–∞–π–¥–µ–Ω–æ –∫–∞—Ä—Ç–æ—á–µ–∫ –Ω–∏ –æ–¥–Ω–∏–º —Å–µ–ª–µ–∫—Ç–æ—Ä–æ–º")
                # –ü–æ–ø—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ –ª—é–±—ã–µ —Å—Å—ã–ª–∫–∏ –Ω–∞ –ª–æ—Ç—ã
                all_links = await page.query_selector_all('a[href*="/en/l/"]')
                print(f"[{time.strftime('%H:%M:%S')}] –í—Å–µ–≥–æ —Å—Å—ã–ª–æ–∫ –Ω–∞ /en/l/: {len(all_links)}")

                for link in all_links:
                    href = await link.get_attribute('href')
                    if href and '/en/l/' in href:
                        if href.startswith('http'):
                            lot_url = href.split('?')[0]
                        else:
                            lot_url = f"https://www.catawiki.com{href.split('?')[0]}"
                        if lot_url not in lot_urls:
                            lot_urls.append(lot_url)

                return lot_urls

            # –ï—Å–ª–∏ –Ω–∞—à–ª–∏ –∫–∞—Ä—Ç–æ—á–∫–∏, –∏–∑–≤–ª–µ–∫–∞–µ–º URL
            for card in lot_cards:
                # –ü–æ–ø—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ —Å—Å—ã–ª–∫—É
                if await card.get_attribute('href'):
                    # –≠—Ç–æ —Å–∞–º–∞ —Å—Å—ã–ª–∫–∞
                    href = await card.get_attribute('href')
                else:
                    # –ò—â–µ–º —Å—Å—ã–ª–∫—É –≤–Ω—É—Ç—Ä–∏
                    link = await card.query_selector('a[href*="/en/l/"]')
                    if link:
                        href = await link.get_attribute('href')
                    else:
                        continue

                if href:
                    # –ü–æ–ª–Ω—ã–π URL
                    if href.startswith('http'):
                        lot_url = href.split('?')[0]  # –£–±—Ä–∞—Ç—å query –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
                    else:
                        lot_url = f"https://www.catawiki.com{href.split('?')[0]}"

                    lot_urls.append(lot_url)

        except Exception as e:
            print(f"[{time.strftime('%H:%M:%S')}] ‚ùå –û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è URL: {e}")
            import traceback
            traceback.print_exc()

        return lot_urls

    async def get_total_pages(self, page) -> int:
        """–û–ø—Ä–µ–¥–µ–ª–∏—Ç—å –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü –≤ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏"""
        try:
            # –ù–∞–π—Ç–∏ –Ω–∞–≤–∏–≥–∞—Ü–∏—é –ø–∞–≥–∏–Ω–∞—Ü–∏–∏
            pagination = await page.query_selector('nav.c-pagination__container')
            if not pagination:
                return 1

            # –ù–∞–π—Ç–∏ –≤—Å–µ –Ω–æ–º–µ—Ä–∞ —Å—Ç—Ä–∞–Ω–∏—Ü
            page_numbers = await pagination.query_selector_all('[data-testid="page"]')

            max_page = 1
            for page_elem in page_numbers:
                text = await page_elem.inner_text()
                text = text.strip()

                # –ü—Ä–æ–ø—É—Å—Ç–∏—Ç—å "..."
                if text == '‚Ä¶' or text == '...':
                    continue

                try:
                    page_num = int(text)
                    if page_num > max_page:
                        max_page = page_num
                except ValueError:
                    continue

            print(f"[{time.strftime('%H:%M:%S')}] üìÑ –í—Å–µ–≥–æ —Å—Ç—Ä–∞–Ω–∏—Ü: {max_page}")
            return max_page

        except Exception as e:
            print(f"[{time.strftime('%H:%M:%S')}] ‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü: {e}")
            return 1

    async def scrape_category(self, category_url: str, max_pages: Optional[int] = None) -> List[dict]:
        """
        –ü–∞—Ä—Å–∏–Ω–≥ –≤—Å–µ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ —Å –ø–∞–≥–∏–Ω–∞—Ü–∏–µ–π

        Args:
            category_url: URL –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
            max_pages: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ (None = –≤—Å–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã)

        Returns:
            –°–ø–∏—Å–æ–∫ –¥–∞–Ω–Ω—ã—Ö –≤—Å–µ—Ö –ª–æ—Ç–æ–≤
        """
        print("=" * 70)
        print("üóÇÔ∏è Catawiki Category Scraper")
        print("=" * 70)
        print(f"Category URL: {category_url}")
        print(f"Max pages: {max_pages or 'ALL'}")
        print(f"Time: {time.strftime('%Y-%m-%d %H:%M:%S')}")
        print("=" * 70)

        all_lot_urls = []

        async with async_playwright() as p:
            try:
                # –ó–∞–ø—É—Å—Ç–∏—Ç—å –±—Ä–∞—É–∑–µ—Ä —Å –∞–Ω—Ç–∏-–¥–µ—Ç–µ–∫—Ü–∏–µ–π
                browser = await p.chromium.launch(
                    headless=self.headless,
                    args=[
                        '--disable-blink-features=AutomationControlled',
                        '--disable-dev-shm-usage',
                        '--no-sandbox',
                        '--disable-setuid-sandbox',
                        '--disable-web-security',
                        '--disable-features=IsolateOrigins,site-per-process',
                        '--disable-gpu',
                        '--single-process',
                    ],
                    timeout=30000
                )

                # Create context with stealth settings
                context = await browser.new_context(
                    viewport={'width': 1920, 'height': 1080},
                    user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                    locale='en-US',
                    timezone_id='Europe/Amsterdam',
                    extra_http_headers={
                        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                        'Accept-Language': 'en-US,en;q=0.9',
                        'Accept-Encoding': 'gzip, deflate, br',
                        'Connection': 'keep-alive',
                        'Upgrade-Insecure-Requests': '1',
                    }
                )

                # Add stealth script to hide automation
                await context.add_init_script("""
                    Object.defineProperty(navigator, 'webdriver', {get: () => undefined});
                    Object.defineProperty(navigator, 'plugins', {get: () => [1, 2, 3]});
                    Object.defineProperty(navigator, 'languages', {get: () => ['en-US', 'en']});
                    window.chrome = {runtime: {}};
                """)

                page = await context.new_page()

                # –ó–∞–≥—Ä—É–∑–∏—Ç—å –ø–µ—Ä–≤—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
                print(f"[{time.strftime('%H:%M:%S')}] üåê –ó–∞–≥—Ä—É–∑–∫–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏...")
                response = await page.goto(category_url, wait_until='domcontentloaded', timeout=30000)

                # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –±–ª–æ–∫–∏—Ä–æ–≤–∫—É
                if response and response.status == 403:
                    print(f"[{time.strftime('%H:%M:%S')}] ‚ùå Catawiki –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–ª –¥–æ—Å—Ç—É–ø (403)")
                    await browser.close()
                    return []

                print(f"[{time.strftime('%H:%M:%S')}] ‚úì –°—Ç—Ä–∞–Ω–∏—Ü–∞ –∑–∞–≥—Ä—É–∂–µ–Ω–∞ (—Å—Ç–∞—Ç—É—Å: {response.status if response else 'unknown'})")

                await asyncio.sleep(3)  # –î–∞—Ç—å –≤—Ä–µ–º—è –Ω–∞ –∑–∞–≥—Ä—É–∑–∫—É –∫–æ–Ω—Ç–µ–Ω—Ç–∞

                # –û—Ç–ª–∞–¥–∫–∞: —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å HTML –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
                html_content = await page.content()
                print(f"[{time.strftime('%H:%M:%S')}] üìÑ HTML —Ä–∞–∑–º–µ—Ä: {len(html_content)} —Å–∏–º–≤–æ–ª–æ–≤")

                # –û–ø—Ä–µ–¥–µ–ª–∏—Ç—å –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü
                total_pages = await self.get_total_pages(page)

                if max_pages:
                    total_pages = min(total_pages, max_pages)

                # –ü–∞—Ä—Å–∏–Ω–≥ –∫–∞–∂–¥–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
                for page_num in range(1, total_pages + 1):
                    print(f"\n[{time.strftime('%H:%M:%S')}] üìë –°—Ç—Ä–∞–Ω–∏—Ü–∞ {page_num}/{total_pages}")

                    # –ï—Å–ª–∏ –Ω–µ –ø–µ—Ä–≤–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞, –ø–µ—Ä–µ–π—Ç–∏ –Ω–∞ –Ω—É–∂–Ω—É—é
                    if page_num > 1:
                        # –ü–æ—Å—Ç—Ä–æ–∏—Ç—å URL —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–º page
                        separator = '&' if '?' in category_url else '?'
                        page_url = f"{category_url}{separator}page={page_num}"

                        print(f"[{time.strftime('%H:%M:%S')}] üåê –ü–µ—Ä–µ—Ö–æ–¥ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—É {page_num}...")
                        await page.goto(page_url, wait_until='domcontentloaded', timeout=30000)
                        await asyncio.sleep(3)

                    # –ò–∑–≤–ª–µ—á—å URL –ª–æ—Ç–æ–≤ —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
                    lot_urls = await self.extract_lot_urls_from_page(page)
                    all_lot_urls.extend(lot_urls)

                    print(f"[{time.strftime('%H:%M:%S')}] ‚úì –ò–∑–≤–ª–µ—á–µ–Ω–æ {len(lot_urls)} URL –ª–æ—Ç–æ–≤")

                await browser.close()

            except Exception as e:
                print(f"[{time.strftime('%H:%M:%S')}] ‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏: {e}")
                return []

        # –£–¥–∞–ª–∏—Ç—å –¥—É–±–ª–∏–∫–∞—Ç—ã
        all_lot_urls = list(set(all_lot_urls))
        print(f"\n[{time.strftime('%H:%M:%S')}] üìä –í—Å–µ–≥–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –ª–æ—Ç–æ–≤: {len(all_lot_urls)}")

        # –¢–µ–ø–µ—Ä—å –ø–∞—Ä—Å–∏–º –∫–∞–∂–¥—ã–π –ª–æ—Ç
        print(f"\n[{time.strftime('%H:%M:%S')}] üöÄ –ù–∞—á–∏–Ω–∞–µ–º –ø–∞—Ä—Å–∏–Ω–≥ –∫–∞–∂–¥–æ–≥–æ –ª–æ—Ç–∞...")
        all_results = []

        for i, lot_url in enumerate(all_lot_urls, 1):
            print(f"\n[{time.strftime('%H:%M:%S')}] üì¶ –õ–æ—Ç {i}/{len(all_lot_urls)}: {lot_url}")

            try:
                # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π scraper –¥–ª—è –ª–æ—Ç–∞
                result = await self.scraper.scrape_listing(lot_url)

                if result and result.get('title'):
                    all_results.append(result)
                    print(f"[{time.strftime('%H:%M:%S')}] ‚úÖ –£—Å–ø–µ—à–Ω–æ —Å–ø–∞—Ä—Å–µ–Ω–æ")
                else:
                    print(f"[{time.strftime('%H:%M:%S')}] ‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Å–ø–∞—Ä—Å–∏—Ç—å –ª–æ—Ç")

            except Exception as e:
                print(f"[{time.strftime('%H:%M:%S')}] ‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –ª–æ—Ç–∞: {e}")
                continue

            # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
            if i < len(all_lot_urls):
                await asyncio.sleep(3)

        print("\n" + "=" * 70)
        print(f"‚úÖ –ü–∞—Ä—Å–∏–Ω–≥ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –∑–∞–≤–µ—Ä—à–µ–Ω!")
        print(f"–í—Å–µ–≥–æ –ª–æ—Ç–æ–≤ –Ω–∞–π–¥–µ–Ω–æ: {len(all_lot_urls)}")
        print(f"–£—Å–ø–µ—à–Ω–æ —Å–ø–∞—Ä—Å–µ–Ω–æ: {len(all_results)}")
        print(f"–ü—Ä–æ–≤–∞–ª–µ–Ω–æ: {len(all_lot_urls) - len(all_results)}")
        print("=" * 70)

        return all_results


async def main():
    """–ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è"""
    import sys
    import json

    if len(sys.argv) < 2:
        print("Usage: python category_scraper.py <category_url> [max_pages]")
        print("\nExample:")
        print('  python category_scraper.py "https://www.catawiki.com/en/s?q=burgundy&filters=..." 2')
        sys.exit(1)

    category_url = sys.argv[1]
    max_pages = int(sys.argv[2]) if len(sys.argv) > 2 else None

    scraper = CatawikiCategoryScraper(headless=True)
    results = await scraper.scrape_category(category_url, max_pages=max_pages)

    # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    if results:
        output_file = f"category_results_{int(time.time())}.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)

        print(f"\nüíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {output_file}")


if __name__ == "__main__":
    asyncio.run(main())
